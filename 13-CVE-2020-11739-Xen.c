///usr/local/clang-4.0/bin/clang -emit-llvm -g -c  13-CVE-2020-11739-Xen.c -o 13-CVE-2020-11739-Xen.bc
// Created by nsas2020 on 24-3-16.
//https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11739
//http://xenbits.xen.org/xsa/advisory-314.html
//
#include <stdint.h>
#include <stdio.h>
#include <pthread.h>
#include <unistd.h>
#include <stdatomic.h>


typedef struct {
    atomic_int slock;
} spinlock_t;

typedef struct {
    atomic_int cnts;
    spinlock_t lock;
} rwlock_t;

#define _QW_WAITING 1            /* A writer is waiting */
#define _QW_LOCKED  0xff         /* A writer holds the lock */
#define _QW_WMASK   0xff         /* Writer mask */
#define _QR_SHIFT   8            /* Reader count shift */
#define _QR_BIAS    (1U << _QR_SHIFT)

void atomic_sub(atomic_int *v, int i) {
    atomic_fetch_sub(v, i);
    //atomic_fetch_sub_explicit(v, i, memory_order_release);

}

static inline int atomic_read(atomic_int *v) {
    return (int)atomic_load_explicit(v, memory_order_acquire);
}

static inline int atomic_cmpxchg(atomic_int *v, int old, int new1) {
    atomic_compare_exchange_strong(v, &old, new1);
    return old;
}

void spin_lock(spinlock_t *lock) {
    while (atomic_exchange(&lock->slock, 0) != 1) {
        // busy-wait
    }
}

void spin_unlock(spinlock_t *lock) {
    atomic_store(&lock->slock, 1);
}

void queue_write_lock_slowpath(rwlock_t *lock) {
    spin_lock(&lock->lock);

    if (atomic_read(&lock->cnts) == 0 && atomic_cmpxchg(&lock->cnts, 0, _QW_LOCKED) == 0) {
        spin_unlock(&lock->lock);
        return;
    }

    for (;;) {
        int cnts = atomic_read(&lock->cnts);
        if (!(cnts & _QW_WMASK) && atomic_cmpxchg(&lock->cnts, cnts, cnts | _QW_WAITING) == cnts) {
            break;
        }
        // CPU relaxation can be implemented if needed
    }

    for (;;) {
        int cnts = atomic_read(&lock->cnts);
        if (cnts == _QW_WAITING && atomic_cmpxchg(&lock->cnts, _QW_WAITING, _QW_LOCKED) == _QW_WAITING) {
            break;
        }
        // CPU relaxation can be implemented if needed
    }

    spin_unlock(&lock->lock);
}


void _write_lock(rwlock_t *lock) {
    /*
* Optimize for the unfair lock case where the fair flag is 0.
*
* atomic_cmpxchg() is a full barrier so no need for an
* arch_lock_acquire_barrier().
*/
    if (atomic_cmpxchg(&lock->cnts, 0, _QW_LOCKED) == 0)
        return;

    /*
     * queue_write_lock_slowpath() is using spinlock and therefore is a
     * full barrier. So no need for an arch_lock_acquire_barrier().
     */
    queue_write_lock_slowpath(lock);
}

void _write_unlock(rwlock_t *lock) {
   //arch_lock_release_barrier();
    atomic_sub(&lock->cnts, _QW_LOCKED);
}

void *thread_function(void *arg) {
    rwlock_t *lock = (rwlock_t *) arg;
    printf("Thread %lu is trying to acquire the lock.\n", (unsigned long) pthread_self());
    _write_lock(lock);
    printf("Thread %lu has acquired the lock.\n", (unsigned long) pthread_self());
    sleep(1); // Simulate data modification.
    _write_unlock(lock);
    printf("Thread %lu has released the lock.\n", (unsigned long) pthread_self());
    return NULL;
}

int main() {
    rwlock_t lock = {.cnts = ATOMIC_VAR_INIT(0), .lock = {.slock = ATOMIC_VAR_INIT(1)}};
    pthread_t threads[2];

    for (int i = 0; i < 2; ++i) {
        pthread_create(&threads[i], NULL, thread_function, &lock);
    }

    for (int i = 0; i < 2; ++i) {
        pthread_join(threads[i], NULL);
    }

    return 0;
}

///usr/local/clang-4.0/bin/clang -emit-llvm -g -c  13-CVE-2020-11739-Xen.c -o 13-CVE-2020-11739-Xen.bc
// Created by nsas2020 on 24-3-16. （41，52）
//https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11739
//http://xenbits.xen.org/xsa/advisory-314.html
// clang -emit-llvm -S -o 13-CVE-2020-11739-Xen.ll 13-CVE-2020-11739-Xen.c
// time nidhugg -arm 13-CVE-2020-11739-Xen.ll
#include <stdint.h>
#include <assert.h>
#include <pthread.h>
#include <unistd.h>
#include <stdatomic.h>


typedef struct {
    atomic_int slock;
} spinlock_t;

typedef struct {
    atomic_int cnts;
    spinlock_t lock;
} rwlock_t;

#define _QW_WAITING 1            /* A writer is waiting */
#define _QW_LOCKED  0xff         /* A writer holds the lock */
#define _QW_WMASK   0xff         /* Writer mask */
#define _QR_SHIFT   8            /* Reader count shift */
#define _QR_BIAS    (1U << _QR_SHIFT)
atomic_int expected = 0;
rwlock_t lock1 = {.cnts = ATOMIC_VAR_INIT(0), .lock = {.slock = ATOMIC_VAR_INIT(1)}};
int temp = 0;



void write_lock(rwlock_t *lock) {
    /*
* Optimize for the unfair lock case where the fair flag is 0.
*
* atomic_cmpxchg() is a full barrier so no need for an
* arch_lock_acquire_barrier().
*/
//    if (atomic_compare_exchange_strong(&lock->cnts, &expected, _QW_LOCKED) == 0)
//        return;
    if (lock->cnts == expected) {
        lock->cnts = _QW_LOCKED;
    } else {
        return;
    }
    /*
     * queue_write_lock_slowpath() is using spinlock and therefore is a
     * full barrier. So no need for an arch_lock_acquire_barrier().
     */
}

void write_unlock(rwlock_t *lock) {
   //arch_lock_release_barrier();
    assert(temp == 1);

    lock->cnts = 0;
}


void *process(void *arg) {
    write_lock(&lock1);
    temp =1;
    // Maybe wait a bit to ensure lock_thread has run
    write_unlock(&lock1);
    return NULL;
}

int main() {
    pthread_t thread1;

    pthread_create(&thread1, NULL, process, NULL);

    pthread_join(thread1, NULL);

    return 0;
}